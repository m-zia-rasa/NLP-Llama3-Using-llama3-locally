{"cells":[{"cell_type":"markdown","metadata":{"id":"XnHdzcrLCbWZ"},"source":["# Usinging Llama3 Locally\n","=============================\n","\n","    *M. Zia Rasa*\n","\n","* Note 1: Using Llama3 locally is recommonded if you have a powerful GPU installed on your PC.\n","* Note2: A C++ compiler or Visuall Studio Development Tools must be installed on your PC too.\n","\n","## Table of Contents\n","\n","1. **Setting up Llama3 locally**\n","2. **Working with \"llama_cpp\" locally**\n","3. **Generating text or Completion with Llama3**\n","    * 1. Basic Completion\n","    * 2. Streaming Completion\n","    * 3. Chat completion\n","4. **Setting Some Hyperparameter**\n","    * 1. Setting some input pararameters\n","    * 2. Setting some output parameters\n","    * 3. tempratures\n","    * 4. top-k\n","    * 5. top-p\n","5. **Generate Embeddings**\n","6. **Summary**"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Setting up Llama3 locally"]},{"cell_type":"markdown","metadata":{"id":"JpHY3IyViD-n"},"source":["### Step 1: \n","You need to install the \"llama-cpp\" by:\n","          \"pip install llama-cpp-python\"\n","which is a C++ class that interacts with Llama3 locally. "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"uxBYCU_Tn1-6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: llama-cpp-python in c:\\users\\rasam\\desktop\\python machine learning projects\\.venv\\lib\\site-packages (0.3.1)\n","Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\rasam\\desktop\\python machine learning projects\\.venv\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n","Requirement already satisfied: numpy>=1.20.0 in c:\\users\\rasam\\desktop\\python machine learning projects\\.venv\\lib\\site-packages (from llama-cpp-python) (2.1.2)\n","Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\rasam\\desktop\\python machine learning projects\\.venv\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n","Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\rasam\\desktop\\python machine learning projects\\.venv\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rasam\\desktop\\python machine learning projects\\.venv\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.1)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install llama-cpp-python --upgrade"]},{"cell_type":"markdown","metadata":{},"source":["### Step 2:\n","import \"llama-cpp\" package and and check out its version."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["'0.3.1'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import llama_cpp\n","llama_cpp.__version__"]},{"cell_type":"markdown","metadata":{},"source":["### Step 3:\n","1. You need to download the \"Meta-Llama-3-8B-Instruct-GGUF\" file form below link:\n","        \"https://huggingface.co/PawanKrd/Meta-Llama-3-8B-Instruct-GGUF/tree/main\"\n","there are different vesions with different sizes (2GB-8GB), download your desired vesion.\n","\n","2. Then set up the \"Meta-Llama-3-8B-Instruct-GGUF\" file path directory locally on your machine."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from llama_cpp import Llama\n","path_to_model=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\""]},{"cell_type":"markdown","metadata":{},"source":["### Step 4:\n","Check out if the llama3 file is set up correclty.\n","if it is set up correclty, you will get an output response"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./Llama3-model/llama-3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = workspace\n","llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n","llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n","llama_model_loader: - kv  12:                          general.file_type u32              = 7\n","llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n","llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n","llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q8_0:  226 tensors\n","llm_load_vocab: missing pre-tokenizer type, using: 'default'\n","llm_load_vocab:                                             \n","llm_load_vocab: ************************************        \n","llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n","llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n","llm_load_vocab: ************************************        \n","llm_load_vocab:                                             \n","llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n","llm_load_vocab: special tokens cache size = 256\n","llm_load_vocab: token to piece cache size = 0.8000 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = BPE\n","llm_load_print_meta: n_vocab          = 128256\n","llm_load_print_meta: n_merges         = 280147\n","llm_load_print_meta: vocab_only       = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_swa            = 0\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 500000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_ctx_orig_yarn  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: ssm_dt_b_c_rms   = 0\n","llm_load_print_meta: model type       = 8B\n","llm_load_print_meta: model ftype      = Q8_0\n","llm_load_print_meta: model params     = 8.03 B\n","llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n","llm_load_print_meta: general.name     = workspace\n","llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n","llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n","llm_load_print_meta: LF token         = 128 'Ä'\n","llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: EOG token        = 128001 '<|end_of_text|>'\n","llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: max token length = 256\n","llm_load_tensors: ggml ctx size =    0.14 MiB\n","llm_load_tensors:        CPU buffer size =  8137.64 MiB\n",".........................................................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 500000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n","llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'general.name': 'workspace', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.vocab_size': '128256', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n","Available chat formats from metadata: chat_template.default\n","Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n","\n","'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n","\n","' }}\n","Using chat eos_token: <|end_of_text|>\n","Using chat bos_token: <|begin_of_text|>\n","llama_perf_context_print:        load time =    3060.01 ms\n","llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n","llama_perf_context_print:       total time =   10425.90 ms /    21 tokens\n"]},{"name":"stdout","output_type":"stream","text":["{'id': 'cmpl-6d82dee8-4b10-43e8-8ced-2012fc49c50f', 'object': 'text_completion', 'created': 1728640734, 'model': './Llama3-model/llama-3-8b-instruct.Q8_0.gguf', 'choices': [{'text': ' - Llamas are originally from South America, particularly in the Andean region', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 6, 'completion_tokens': 16, 'total_tokens': 22}}\n"]}],"source":["llm=Llama(model_path=path_to_model)\n","output = llm (\"where does llamas live?\")\n","print(output)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Working with \"llama_cpp\" locally\n","    * Now enjoy working with llama3 language model locally"]},{"cell_type":"markdown","metadata":{},"source":["## Generating text or completion with Llama3\n","\n","**Types of completions with Llama3**\n","1. Basic completion\n","2. Streaming completion\n","3. Chat completion\n","      * Simple chat completion\n","      * Json mode chat completion\n","      * Json schema mode chat completion\n","      * Creating an LLM interface\n","      * Creating an agent class"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Basic completion"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["<llama_cpp.llama.Llama at 0x2dc3be0ee90>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from llama_cpp import Llama\n","\n","llm = Llama(model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\", verbose=False)\n","\n","llm"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["resp = llm(\"Q: Write a short paragraph introducing Elon Musk. A: \",\n","           max_tokens=256,\n","           stop=[\"Q:\", \"\\n\"])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["{'id': 'cmpl-e4606afa-9614-4ec9-a2f3-39fc47a5541a',\n"," 'object': 'text_completion',\n"," 'created': 1728640750,\n"," 'model': './Llama3-model/llama-3-8b-instruct.Q8_0.gguf',\n"," 'choices': [{'text': \"\\xa0Elon Musk is a South African-born entrepreneur and business magnate who has revolutionized the way we live and work. With a net worth of over $200 billion, Musk is one of the richest people in the world. He is best known for his innovative ventures, including SpaceX, Tesla, and Neuralink, which are pushing the boundaries of technology and space exploration. Musk's vision is to make humanity a multi-planetary species and to accelerate the world's transition to sustainable energy. Through his companies, he has already achieved significant milestones, such as developing the electric car, pioneering private space travel, and making solar energy more accessible. With his relentless pursuit of innovation, Musk continues to inspire and captivate the world. ...read more\",\n","   'index': 0,\n","   'logprobs': None,\n","   'finish_reason': 'stop'}],\n"," 'usage': {'prompt_tokens': 13, 'completion_tokens': 149, 'total_tokens': 162}}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["resp"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["\"\\xa0Elon Musk is a South African-born entrepreneur and business magnate who has revolutionized the way we live and work. With a net worth of over $200 billion, Musk is one of the richest people in the world. He is best known for his innovative ventures, including SpaceX, Tesla, and Neuralink, which are pushing the boundaries of technology and space exploration. Musk's vision is to make humanity a multi-planetary species and to accelerate the world's transition to sustainable energy. Through his companies, he has already achieved significant milestones, such as developing the electric car, pioneering private space travel, and making solar energy more accessible. With his relentless pursuit of innovation, Musk continues to inspire and captivate the world. ...read more\""]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["resp[\"choices\"][0][\"text\"]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Name 5 species of llamas?\n"]}],"source":["output = llm (\n","    \"Q:Name 5 species of llamas? A:\",\n","    max_tokens=32,\n","    stop=[\"Q:\",\"\\n\"],\n","    )\n","print(output ['choices'][0]['text'])"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Streaming completion of response"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["<generator object Llama._create_completion at 0x000002DC3B484550>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from llama_cpp import Llama\n","llm = Llama(model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\", verbose=False)\n","\n","resp = llm(\"Q: Write a short paragraph introducing Elon Musk. A: \",\n","           max_tokens=256,\n","           stop=[\"Q:\", \"\\n\"],\n","           stream=True)\n","\n","resp"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" Elon Musk is a South African-born entrepreneur and business magnate who has revolutionized the way we live and work. With a net worth of over $200 billion, Musk is one of the richest people in the world. He is best known for his innovative ventures, including SpaceX, Tesla, and Neuralink, which are pushing the boundaries of technology and space exploration. Musk's vision is to make humanity a multi-planetary species and to accelerate the world's transition to sustainable energy. Through his companies, he has already achieved significant milestones, such as developing the electric car, pioneering private space travel, and making solar energy more accessible. With his relentless pursuit of innovation, Musk continues to inspire and captivate the world. ...read more"]}],"source":["for r in resp:\n","    print(r[\"choices\"][0][\"text\"], end=\"\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" To improve your English, I would recommend the following:"]}],"source":["output = llm (\n","    \"Q: How to imporve my English? A:\",\n","    max_tokens=128,\n","    stop=[\"Q:\",\"\\n\"],\n","    stream=True,\n","    )\n","for token in output:\n","    print(token['choices'][0]['text'], end='')"]},{"cell_type":"markdown","metadata":{},"source":["### Chat Completion"]},{"cell_type":"markdown","metadata":{},"source":["1. Simple chat completion"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from llama_cpp import Llama\n","llm = Llama(model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\", verbose=False)\n","\n","output = llm.create_chat_completion(\n","    messages =[\n","        {\"role\":\"system\",\n","         \"content\":\"you are an assistan who speaks only Shakespearean\"\n","        },\n","        {\n","            \"role\":\"user\",\n","            \"content\":\"Dercribe New York in 10 words\"\n","        }\n","    ]\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\"Fairest New York, thy steel canyons touch the heavens.\"\n"]}],"source":["print(output['choices'][0]['message']['content'])"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["{'id': 'chatcmpl-161126b4-3b0a-45c2-8228-77be131704a5',\n"," 'object': 'chat.completion',\n"," 'created': 1728640933,\n"," 'model': './Llama3-model/llama-3-8b-instruct.Q8_0.gguf',\n"," 'choices': [{'index': 0,\n","   'message': {'role': 'assistant',\n","    'content': 'Elon Musk is a visionary entrepreneur, inventor, and business magnate who embodies the essence of innovation and disruption. With his unyielding passion for pushing boundaries and challenging the status quo, Musk has revolutionized the electric car industry with Tesla, made space exploration accessible with SpaceX, and transformed the way we communicate with Neuralink. His unparalleled drive, unwavering dedication, and relentless pursuit of progress have earned him a reputation as a trailblazing pioneer, with a presence that is both imposing and charismatic. With his sharp mind and quick wit, Musk navigates the complexities of the 21st century with ease, leaving a trail of groundbreaking achievements that continue to redefine the very fabric of modern society.'},\n","   'logprobs': None,\n","   'finish_reason': 'stop'}],\n"," 'usage': {'prompt_tokens': 34, 'completion_tokens': 141, 'total_tokens': 175}}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["resp = llm.create_chat_completion(\n","      messages = [\n","          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes individuals.\"},\n","          {\n","              \"role\": \"user\",\n","              \"content\": \"Write a short paragraph introducing Elon Musk.\"\n","          }\n","      ]\n",")\n","\n","resp"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["{'role': 'assistant',\n"," 'content': 'Elon Musk is a visionary entrepreneur, inventor, and business magnate who embodies the essence of innovation and disruption. With his unyielding passion for pushing boundaries and challenging the status quo, Musk has revolutionized the electric car industry with Tesla, made space exploration accessible with SpaceX, and transformed the way we communicate with Neuralink. His unparalleled drive, unwavering dedication, and relentless pursuit of progress have earned him a reputation as a trailblazing pioneer, with a presence that is both imposing and charismatic. With his sharp mind and quick wit, Musk navigates the complexities of the 21st century with ease, leaving a trail of groundbreaking achievements that continue to redefine the very fabric of modern society.'}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["resp[\"choices\"][0][\"message\"]"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["{'id': 'chatcmpl-b2e18bb8-d7bd-478e-b26e-701dc900a9d0',\n"," 'object': 'chat.completion',\n"," 'created': 1728641005,\n"," 'model': './Llama3-model/llama-3-8b-instruct.Q8_0.gguf',\n"," 'choices': [{'index': 0,\n","   'message': {'role': 'assistant',\n","    'content': 'Here is a revised paragraph introducing Elon Musk:\\n\\nElon Musk is a trailblazing entrepreneur and inventor who has made a profound impact on the world through his innovative ventures. With a relentless drive to push the boundaries of what is possible, Musk has disrupted industries and transformed the way we live, work, and explore. As the co-founder of SpaceX, he has made space travel more accessible, and as the CEO of Tesla, he has revolutionized the electric car industry. With his sharp intellect and entrepreneurial spirit, Musk has built a reputation as a visionary leader, and his influence continues to shape the future of technology and beyond.'},\n","   'logprobs': None,\n","   'finish_reason': 'stop'}],\n"," 'usage': {'prompt_tokens': 196,\n","  'completion_tokens': 125,\n","  'total_tokens': 321}}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["new_resp = llm.create_chat_completion(\n","      messages = [\n","          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes individuals.\"},\n","          {\n","              \"role\": \"user\",\n","              \"content\": \"Write a short paragraph introducing Elon Musk.\"\n","          },\n","          resp[\"choices\"][0][\"message\"],\n","          {\n","              \"role\": \"user\",\n","              \"content\": \"Can you please rephrase your previous response?\"\n","          }\n","      ]\n",")\n","\n","new_resp"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["{'role': 'assistant',\n"," 'content': 'Here is a revised paragraph introducing Elon Musk:\\n\\nElon Musk is a trailblazing entrepreneur and inventor who has made a profound impact on the world through his innovative ventures. With a relentless drive to push the boundaries of what is possible, Musk has disrupted industries and transformed the way we live, work, and explore. As the co-founder of SpaceX, he has made space travel more accessible, and as the CEO of Tesla, he has revolutionized the electric car industry. With his sharp intellect and entrepreneurial spirit, Musk has built a reputation as a visionary leader, and his influence continues to shape the future of technology and beyond.'}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["new_resp[\"choices\"][0][\"message\"]"]},{"cell_type":"markdown","metadata":{},"source":["2. Jsome mode chat completion"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","\"names\": [\n","\"Roger Penrose\",\n","\"Andrea Ghez\",\n","\"Reinhard Genzel\"\n","]\n","}\n"]}],"source":["from llama_cpp import Llama\n","llm = Llama(model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\", verbose=False)\n","\n","output = llm.create_chat_completion(\n","    messages =[\n","        {\"role\":\"system\",\n","         \"content\":\"you are a helpful assistant that outputs in JSON\"\n","        },\n","        {\n","            \"role\":\"user\",\n","            \"content\":\"Who won the 2020 Nobel prize in pyysic? Provie a list in 'names'\"\n","        },\n","    ],\n","    response_format={\"type\":\"json_objects\",}\n",")\n","print(output['choices'][0]['message']['content'])"]},{"cell_type":"markdown","metadata":{},"source":["3. Json schema mode chat completion"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Here is the information you requested:\n","\n","```json\n","{\n","    \"2020 Nobel Prize in Physics\": {\n","        \"names\": [\"Roger Penrose\", \"Andrea Ghez\", \"Reinhard Genzel\"]\n","    }\n","}\n","```\n"]}],"source":["output = llm.create_chat_completion(\n","    messages =[\n","        {\"role\":\"system\",\n","         \"content\":\"you are a helpful assistant that outputs in JSON\"\n","        },\n","        {\n","            \"role\":\"user\",\n","            \"content\":\"Who won the 2020 Nobel prize in pyysic? Provie a list in 'names'\"\n","        },\n","    ],\n","\n","    response_format={\"type\":\"json_objects\",\n","                     \"schema\":{\n","                         \"type\":\"object\",\n","                         \"properties\":{\"prize_type\":{\"type\":\"string\"}},\n","                         \"name\": {\"type\":\"string\"}},\n","                     \"required\":[\"prize_type\",\"name\"]}\n",")\n","print(output['choices'][0]['message']['content'])"]},{"cell_type":"markdown","metadata":{},"source":["4. Creating  an LLM interface class"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["I think there may be a small mistake! You asked about the Nobel Prize in Physics, not \"pyysic\".\n","\n","Here is the correct answer:\n","\n","{\n","\"names\": [\n","\"Roger Penrose\",\n","\"Reinhard Genzel\",\n","\"Andrea Ghez\"\n","]\n","}\n","\n","These three scientists won the 2020 Nobel Prize in Physics for their discoveries about black holes, particularly the discovery that black holes have a property known as \"event horizons\".\n"]}],"source":["from llama_cpp import Llama\n","llm = Llama(model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\", verbose=False)\n","\n","output = llm.create_chat_completion(\n","    messages =[\n","        {\"role\":\"system\",\n","         \"content\":\"you are a helpful assistant that outputs in JSON\"\n","        },\n","        {\n","            \"role\":\"user\",\n","            \"content\":\"Who won the 2020 Nobel prize in pyysic? Provie a list in 'names'\"\n","        },\n","        {\n","             \"role\":\"assistant\",\n","            \"content\":\"Ti is a beautiful building\"\n","        },\n","    ],\n","    response_format={\"type\":\"json_objects\",}\n",")\n","print(output['choices'][0]['message']['content'])"]},{"cell_type":"markdown","metadata":{},"source":["5. Creating an agent class"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["class Agent:\n","    def __init__(self, llm, system_prompt: str = '', history: list = []):\n","        self.llm = llm\n","        self.system_prompt = system_prompt\n","        self.history = [{\"role\": \"system\", \"content\": self.system_prompt}] + history\n","\n","    def create_completion(self, user_prompt: str = '', max_tokens: int = 20):\n","        self.history.append({\"role\": \"user\", \"content\": user_prompt})\n","        output = self.llm.create_chat_completion(messages=self.history, max_tokens=max_tokens)\n","        agent_result = output['choices'][0]['message']\n","        self.history.append(agent_result)\n","        return agent_result['content']"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fair Paris, thou city of love and of woe,\n","Whereon doth stand the Eiff\n"]}],"source":["agent = Agent(llm,\n","              system_prompt=\"You only speak in the voice of Shakespeare\")\n","rest= agent.create_completion (\"Describe the Eiffel tower\")\n","print(rest)"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Some Hyperparameter"]},{"cell_type":"markdown","metadata":{},"source":["1. Setting some input parameters\n","* n_gpu_layers\n","* seed\n","* n_ctx"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":509},"executionInfo":{"elapsed":1104,"status":"error","timestamp":1728114337576,"user":{"displayName":"M Zia Rasa","userId":"04100332887334074153"},"user_tz":-210},"id":"hoeCjSLll8_r","outputId":"befe076b-8847-446b-e5fd-a0cdf8ca07cc"},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./Llama3-model/llama-3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = workspace\n","llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n","llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n","llama_model_loader: - kv  12:                          general.file_type u32              = 7\n","llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n","llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n","llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q8_0:  226 tensors\n","llm_load_vocab: missing pre-tokenizer type, using: 'default'\n","llm_load_vocab:                                             \n","llm_load_vocab: ************************************        \n","llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n","llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n","llm_load_vocab: ************************************        \n","llm_load_vocab:                                             \n","llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n","llm_load_vocab: special tokens cache size = 256\n","llm_load_vocab: token to piece cache size = 0.8000 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = BPE\n","llm_load_print_meta: n_vocab          = 128256\n","llm_load_print_meta: n_merges         = 280147\n","llm_load_print_meta: vocab_only       = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_swa            = 0\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 500000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_ctx_orig_yarn  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: ssm_dt_b_c_rms   = 0\n","llm_load_print_meta: model type       = 8B\n","llm_load_print_meta: model ftype      = Q8_0\n","llm_load_print_meta: model params     = 8.03 B\n","llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n","llm_load_print_meta: general.name     = workspace\n","llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n","llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n","llm_load_print_meta: LF token         = 128 'Ä'\n","llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: EOG token        = 128001 '<|end_of_text|>'\n","llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: max token length = 256\n","llm_load_tensors: ggml ctx size =    0.14 MiB\n","llm_load_tensors:        CPU buffer size =  8137.64 MiB\n",".........................................................................................\n","llama_new_context_with_model: n_ctx      = 2048\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 500000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n","llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'general.name': 'workspace', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.vocab_size': '128256', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n","Available chat formats from metadata: chat_template.default\n","Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n","\n","'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n","\n","' }}\n","Using chat eos_token: <|end_of_text|>\n","Using chat bos_token: <|begin_of_text|>\n"]}],"source":["from llama_cpp import Llama\n","llm = Llama(\n","    model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\",\n","    n_gpu_layers=-1,\n","    seed=1337,\n","    n_ctx=2048,\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["2. Setting some output parameters\n","* max_tokens\n","* stop"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"IxaiQ_lCpfBp"},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_perf_context_print:        load time =    3294.63 ms\n","llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n","llama_perf_context_print:       total time =    7435.90 ms /    21 tokens\n"]},{"name":"stdout","output_type":"stream","text":["Name 5 species of llamas? "]}],"source":["output = llm (\n","    \"Q:Name 5 species of llamas? A:\",\n","    max_tokens=32,\n","    stop=[\"Q:\",\"\\n\"],\n","    )\n","\n","print(output[\"choices\"][0][\"text\"], end=\"\")"]},{"cell_type":"markdown","metadata":{},"source":["3. Temprature"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" **What is the universe?** The universe is all of the matter and energy that exists in space\n"]}],"source":["from llama_cpp import Llama\n","llm = Llama(model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\", verbose=False)\n","\n","output = llm (\n","    \" Descripbe the universe.\",\n","    max_tokens=20,\n","    temperature =1.5,\n",")\n","print(output['choices'][0]['text'])"]},{"cell_type":"markdown","metadata":{},"source":["4. Top-k"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./Llama3-model/llama-3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = workspace\n","llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n","llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n","llama_model_loader: - kv  12:                          general.file_type u32              = 7\n","llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n","llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n","llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q8_0:  226 tensors\n","llm_load_vocab: missing pre-tokenizer type, using: 'default'\n","llm_load_vocab:                                             \n","llm_load_vocab: ************************************        \n","llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n","llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n","llm_load_vocab: ************************************        \n","llm_load_vocab:                                             \n","llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n","llm_load_vocab: special tokens cache size = 256\n","llm_load_vocab: token to piece cache size = 0.8000 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = BPE\n","llm_load_print_meta: n_vocab          = 128256\n","llm_load_print_meta: n_merges         = 280147\n","llm_load_print_meta: vocab_only       = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_swa            = 0\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 500000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_ctx_orig_yarn  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: ssm_dt_b_c_rms   = 0\n","llm_load_print_meta: model type       = 8B\n","llm_load_print_meta: model ftype      = Q8_0\n","llm_load_print_meta: model params     = 8.03 B\n","llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n","llm_load_print_meta: general.name     = workspace\n","llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n","llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n","llm_load_print_meta: LF token         = 128 'Ä'\n","llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: EOG token        = 128001 '<|end_of_text|>'\n","llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: max token length = 256\n","llm_load_tensors: ggml ctx size =    0.14 MiB\n","llm_load_tensors:        CPU buffer size =  8137.64 MiB\n",".........................................................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 500000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n","llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'general.name': 'workspace', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.vocab_size': '128256', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n","Available chat formats from metadata: chat_template.default\n","Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n","\n","'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n","\n","' }}\n","Using chat eos_token: <|end_of_text|>\n","Using chat bos_token: <|begin_of_text|>\n","llama_perf_context_print:        load time =    2828.61 ms\n","llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n","llama_perf_context_print:       total time =   11810.44 ms /    26 tokens\n"]},{"name":"stdout","output_type":"stream","text":[" He wrote that the universe was infinite, and that the stars and planets were all part of a vast\n"]}],"source":["from llama_cpp import Llama\n","llm = Llama(model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\", verbose=False)\n","\n","output = llm (\n","    \" Descripbe the universe.\",\n","    max_tokens=20,\n","    top_k=50,\n",")\n","print(output['choices'][0]['text'])"]},{"cell_type":"markdown","metadata":{},"source":["5. Top-p"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" Use a combination of descriptive words and phrases to paint a vivid picture of the universe.\n","The universe is\n"]}],"source":["from llama_cpp import Llama\n","llm = Llama(model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\", verbose=False)\n","\n","output = llm (\n","    \" Descripbe the universe.\",\n","    max_tokens=20,\n","    top_p=0.8,\n",")\n","print(output['choices'][0]['text'])"]},{"cell_type":"markdown","metadata":{},"source":["# Generating Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from llama_cpp import Llama\n","llama2_chat = Llama(model_path=\"./Llama3-model/llama-3-8b-instruct.Q8_0.gguf\", embedding=True, verbose=False)\n","\n","embeddings = llama2_chat.create_embedding(\"Hello, world!\")\n","\n","embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(embeddings[\"data\"][0][\"embedding\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embeddings = llama2_chat.create_embedding([\"Hello, world!\", \"Goodbye, world!\"])\n","\n","len(embeddings[\"data\"]), len(embeddings[\"data\"][0][\"embedding\"]), len(embeddings[\"data\"][1][\"embedding\"])"]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","\n","To sum up, we have learnt how you can access **open source LLMs** on **Local Machine** using Python library **llama-cpp-python**. Its a wrapper around **llama.cpp** library."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN4QoHMcCTn4+D5Q3GzwAhQ","mount_file_id":"1XGU_H-SwseLOHQ_84S0SJP959rnM9LT9","name":"","version":""},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0"}},"nbformat":4,"nbformat_minor":0}
